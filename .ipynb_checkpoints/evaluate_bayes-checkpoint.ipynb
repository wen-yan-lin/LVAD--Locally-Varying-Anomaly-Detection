{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-set: STL-10\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataLoader import importData\n",
    "\n",
    "\n",
    "data_set = 1\n",
    "percentage_anomalies = [0.1, 1, 10, 20, 30]\n",
    "\n",
    "x_train, y_train, x_test, y_test, dataset_name = importData(data_set)\n",
    "#x_train, x_test = pca_transform(x_train, x_test)\n",
    "\n",
    "x_train = np.concatenate([x_train, x_test], axis=0)\n",
    "y_train = np.concatenate([y_train, y_test])\n",
    "num_class = np.max(y_train) + 1\n",
    "print('Number of classes:', num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_folder = '../stored_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 1 / 10 , anon percentage: 0.1 , auroc: 0.9992307692307693\n",
      "class: 1 / 10 , anon percentage: 1 , auroc: 0.9975739644970415\n",
      "class: 1 / 10 , anon percentage: 10 , auroc: 0.865621301775148\n",
      "class: 1 / 10 , anon percentage: 20 , auroc: 0.8722485207100591\n",
      "class: 1 / 10 , anon percentage: 30 , auroc: 0.878534516765286\n",
      "class: 2 / 10 , anon percentage: 0.1 , auroc: 0.9946153846153846\n",
      "class: 2 / 10 , anon percentage: 1 , auroc: 0.9814201183431954\n",
      "class: 2 / 10 , anon percentage: 10 , auroc: 0.3835029585798816\n",
      "class: 2 / 10 , anon percentage: 20 , auroc: 0.38635798816568045\n",
      "class: 2 / 10 , anon percentage: 30 , auroc: 0.241138067061144\n",
      "class: 3 / 10 , anon percentage: 0.1 , auroc: 0.99\n",
      "class: 3 / 10 , anon percentage: 1 , auroc: 0.9905325443786983\n",
      "class: 3 / 10 , anon percentage: 10 , auroc: 0.7643372781065089\n",
      "class: 3 / 10 , anon percentage: 20 , auroc: 0.6024541420118343\n",
      "class: 3 / 10 , anon percentage: 30 , auroc: 0.6445187376725838\n",
      "class: 4 / 10 , anon percentage: 0.1 , auroc: 0.9984615384615385\n",
      "class: 4 / 10 , anon percentage: 1 , auroc: 0.9966272189349112\n",
      "class: 4 / 10 , anon percentage: 10 , auroc: 0.40450295857988167\n",
      "class: 4 / 10 , anon percentage: 20 , auroc: 0.4623017751479289\n",
      "class: 4 / 10 , anon percentage: 30 , auroc: 0.2864043392504931\n",
      "class: 5 / 10 , anon percentage: 0.1 , auroc: 1.0\n",
      "class: 5 / 10 , anon percentage: 1 , auroc: 0.9917751479289941\n",
      "class: 5 / 10 , anon percentage: 10 , auroc: 0.6903727810650888\n",
      "class: 5 / 10 , anon percentage: 20 , auroc: 0.5577455621301775\n",
      "class: 5 / 10 , anon percentage: 30 , auroc: 0.5573037475345168\n",
      "class: 6 / 10 , anon percentage: 0.1 , auroc: 0.9984615384615385\n",
      "class: 6 / 10 , anon percentage: 1 , auroc: 0.995266272189349\n",
      "class: 6 / 10 , anon percentage: 10 , auroc: 0.33104733727810653\n",
      "class: 6 / 10 , anon percentage: 20 , auroc: 0.27931360946745565\n",
      "class: 6 / 10 , anon percentage: 30 , auroc: 0.3142149901380671\n",
      "class: 7 / 10 , anon percentage: 0.1 , auroc: 0.99\n",
      "class: 7 / 10 , anon percentage: 1 , auroc: 0.9845562130177515\n",
      "class: 7 / 10 , anon percentage: 10 , auroc: 0.6103905325443787\n",
      "class: 7 / 10 , anon percentage: 20 , auroc: 0.43126331360946746\n",
      "class: 7 / 10 , anon percentage: 30 , auroc: 0.4518461538461538\n",
      "class: 8 / 10 , anon percentage: 0.1 , auroc: 1.0\n",
      "class: 8 / 10 , anon percentage: 1 , auroc: 0.9998816568047337\n",
      "class: 8 / 10 , anon percentage: 10 , auroc: 0.6051834319526628\n",
      "class: 8 / 10 , anon percentage: 20 , auroc: 0.5931272189349113\n",
      "class: 8 / 10 , anon percentage: 30 , auroc: 0.3903254437869823\n",
      "class: 9 / 10 , anon percentage: 0.1 , auroc: 0.9753846153846154\n",
      "class: 9 / 10 , anon percentage: 1 , auroc: 0.9707692307692308\n",
      "class: 9 / 10 , anon percentage: 10 , auroc: 0.6986686390532544\n",
      "class: 9 / 10 , anon percentage: 20 , auroc: 0.7205384615384616\n",
      "class: 9 / 10 , anon percentage: 30 , auroc: 0.5787396449704142\n",
      "class: 10 / 10 , anon percentage: 0.1 , auroc: 0.8869230769230769\n",
      "class: 10 / 10 , anon percentage: 1 , auroc: 0.9662721893491124\n",
      "class: 10 / 10 , anon percentage: 10 , auroc: 0.7685857988165681\n",
      "class: 10 / 10 , anon percentage: 20 , auroc: 0.7509526627218935\n",
      "class: 10 / 10 , anon percentage: 30 , auroc: 0.5439132149901381\n"
     ]
    }
   ],
   "source": [
    "from shell_anon import NormalizedClusterLearner\n",
    "from normalization import InstanceNormalization, ErgoNormalization, NaiveNormalization, NoNormalization, PreTrainedNormalization\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, pairwise_distances\n",
    "\n",
    "\n",
    "def build_eval_set(x_train, y_train, ind, p_anon):\n",
    "    x_in = x_train[y_train==ind]\n",
    "    x_out = x_train[y_train!=ind]\n",
    "    random.shuffle(x_out)\n",
    "\n",
    "    num_out = int(p_anon/100 * x_in.shape[0])\n",
    "    data = np.concatenate([x_in, x_out[:num_out]], axis=0)\n",
    "    gt = np.zeros(data.shape[0], dtype=int)\n",
    "    gt[:data.shape[0] -num_out] = 1\n",
    "    \n",
    "    return data, gt\n",
    "\n",
    "\n",
    "\n",
    "class AnonEvaluationStatistics():\n",
    "    def __init__(self, \n",
    "                 percentiles =[0.1, 1, 10, 20, 30], \n",
    "                 name = 'unnamed'):\n",
    "        self.percentiles = percentiles\n",
    "        self.name = name\n",
    "        self.auroc = None\n",
    "        self.auprc = None\n",
    "        self.mean_auroc = None\n",
    "        self.mean_auprc = None\n",
    "    \n",
    "    \n",
    "    def eval(self, x_train, y_train, clf, print_summary=True):\n",
    "        num_class = np.max(y_train) + 1\n",
    "        \n",
    "        auroc_scores = np.zeros([num_class, len(self.percentiles)])\n",
    "        auprc_scores = np.zeros([num_class, len(self.percentiles)])\n",
    "\n",
    "        for class_num in range(num_class):\n",
    "            for anon_ind, p_anon in enumerate(self.percentiles):\n",
    "                data, gt = build_eval_set(x_train, y_train, class_num, p_anon)\n",
    "                \n",
    "                clf.fit(data)\n",
    "                score = clf.score(data)\n",
    "                auroc = roc_auc_score(gt, score)\n",
    "                auprc = average_precision_score(gt, score)\n",
    "\n",
    "                \n",
    "                auroc_scores[class_num, anon_ind] = auroc\n",
    "                auprc_scores[class_num, anon_ind] = auprc\n",
    "                \n",
    "                if print_summary:\n",
    "                    print('class:', class_num + 1, '/', num_class,\n",
    "                          ', anon percentage:', p_anon, \n",
    "                          ', auroc:', auroc)\n",
    "        \n",
    "        self.auroc = auroc_scores\n",
    "        self.auprc = auprc_scores\n",
    "                \n",
    "        self.mean_auroc = np.mean(auroc_scores, axis=0)\n",
    "        self.mean_auprc = np.mean(auprc_scores, axis=0)\n",
    "            \n",
    "\n",
    "\n",
    "                \n",
    "    \n",
    "num_clus = 5\n",
    "clf_ergo = NormalizedClusterLearner(num_clus = num_clus, norm = ErgoNormalization())\n",
    "eval_name = dataset_name + '_naive_ergo_normalization_' + str(num_clus)\n",
    "anon_eval = AnonEvaluationStatistics(name = eval_name)\n",
    "anon_eval.eval(x_train, y_train, clf_ergo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 1 / 10 , anon percentage: 0.1 , auroc: 0.0007692307692307692\n",
      "class: 1 / 10 , anon percentage: 1 , auroc: 0.008106508875739644\n",
      "class: 1 / 10 , anon percentage: 10 , auroc: 0.0049526627218934895\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "num_clus = 300\n",
    "clf_ergo = BayesClusterLearner(num_clus = num_clus, norm = InstanceNormalization())\n",
    "eval_name = dataset_name + '_bayes_instance_normalization_' + str(num_clus)\n",
    "anon_eval = AnonEvaluationStatistics(name = eval_name)\n",
    "anon_eval.eval(x_train, y_train, clf_ergo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'normalization.InstanceNormalization'>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(clf.norm.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InstanceNormalization' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-0d2d0b5f7a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'InstanceNormalization' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "clf.norm.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "store_path = store_folder + '/' + eval_name + '.pickle'\n",
    "with open(store_path, 'wb') as file:\n",
    "    pickle.dump(anon_eval, file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(store_path, 'rb') as file:\n",
    "    b = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999941, 0.99997571, 0.98570921, 0.9741688 , 0.96443542],\n",
       "       [0.99999585, 0.99981381, 0.88742899, 0.79310751, 0.62606375],\n",
       "       [0.99999228, 0.99990553, 0.97280129, 0.88393178, 0.85337837],\n",
       "       [0.99999882, 0.99996645, 0.89753218, 0.84220134, 0.6524824 ],\n",
       "       [1.        , 0.99991725, 0.96075206, 0.87181886, 0.81882269],\n",
       "       [0.99999882, 0.99995269, 0.86747757, 0.75494247, 0.68772233],\n",
       "       [0.99999228, 0.99984447, 0.9467382 , 0.80887817, 0.75428585],\n",
       "       [1.        , 0.99999882, 0.94410405, 0.89490554, 0.70199899],\n",
       "       [0.99998085, 0.99969602, 0.96079609, 0.93299763, 0.82296736],\n",
       "       [0.99990781, 0.99964908, 0.97387531, 0.94399604, 0.79003723]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../stored_results/test', anon_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.load('../stored_results/test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<__main__.AnonEvaluationStatistics object at 0x7fd494e8e3a0>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99923077, 0.99923077, 0.86133136, 0.84907396, 0.87331755],\n",
       "       [0.99923077, 0.93526627, 0.36343787, 0.38632249, 0.22253057],\n",
       "       [0.99692308, 0.9952071 , 0.72683432, 0.5299497 , 0.54893097],\n",
       "       [0.99769231, 0.99639053, 0.38034911, 0.39021598, 0.26346351],\n",
       "       [1.        , 0.99863905, 0.66591124, 0.44484911, 0.50415582],\n",
       "       [0.99846154, 0.99781065, 0.32779882, 0.31661243, 0.15192899],\n",
       "       [0.99615385, 0.99710059, 0.59268047, 0.41322189, 0.42093688],\n",
       "       [1.        , 0.99970414, 0.56349704, 0.38806805, 0.3663432 ],\n",
       "       [0.99692308, 0.97384615, 0.6452071 , 0.62826627, 0.54188166],\n",
       "       [0.96461538, 0.94710059, 0.68539349, 0.5778284 , 0.55277318]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_eval.auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999605, 0.99983237, 0.9299021 , 0.81714435, 0.73221026])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(anon_eval.auprc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anon_ind, p_anon in enumerate(percentage_anomalies):\n",
    "    print(anon_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_stats = {'Ergo Normalization Naive': [], 'Instance Normalization Naive': [],\n",
    "             'Instance Normalization Bayes':}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus = 10\n",
    "\n",
    "for in_class in range(num_class):\n",
    "    print('processing class:', in_class)\n",
    "    for p_anon in percentage_anomalies:\n",
    "        \n",
    "        # in_class and out_class instances\n",
    "        x_in = x_train[y_train==ind]\n",
    "        x_out = x_train[y_train!=ind]\n",
    "        random.shuffle(x_out)\n",
    "\n",
    "        num_out = int(p_anon/100 * x_in.shape[0])\n",
    "        print('number of normal:',  x_in.shape[0])\n",
    "        print('number of anomalies:',  num_out)\n",
    "        \n",
    "        data = np.concatenate([x_in, x_out[:num_out]], axis=0)\n",
    "        gt = np.zeros(data.shape[0], dtype=int)\n",
    "        gt[:data.shape[0] -num_out] = 1\n",
    "\n",
    "        clf_ergo = NormalizedClusterLearner(num_clus=num_clus, norm=ErgoNormalization())\n",
    "        clf_ergo.fit(data)\n",
    "        d = clf.score(data)\n",
    "        auroc = roc_auc_score(gt, -d)\n",
    "        print('auroc score:', auroc)\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "    \n",
    "num_out = 100\n",
    "ind = 0\n",
    "num_clus = 5\n",
    "x_in = x_train[y_train==ind]\n",
    "num_in = x_in.shape[0]\n",
    "\n",
    "x_out = x_train[y_train!=ind]\n",
    "random.shuffle(x_out)\n",
    "\n",
    "data = np.concatenate([x_in, x_out[:num_out]], axis=0)\n",
    "gt = np.zeros(data.shape[0], dtype=int)\n",
    "gt[:data.shape[0] -num_out] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620153846153845"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shell_anon import NormalizedClusterLearner, BayesClusterLearner\n",
    "from normalization import InstanceNormalization, ErgoNormalization, NaiveNormalization, NoNormalization, PreTrainedNormalization\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, pairwise_distances\n",
    "\n",
    "clf = NormalizedClusterLearner(num_clus=num_clus)\n",
    "clf.fit(data)\n",
    "d = clf.score(data)\n",
    "\n",
    "roc_auc_score(gt, -d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8609615384615384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NormalizedClusterLearner(num_clus=num_clus, norm=InstanceNormalization())\n",
    "clf.fit(data)\n",
    "d = clf.score(data)\n",
    "\n",
    "roc_auc_score(gt, -d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BayesClusterLearner(num_clus=300, norm=InstanceNormalization())\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = clf.score(data)\n",
    "roc_auc_score(gt, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
